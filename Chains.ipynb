{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba383f1",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fe0b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key:········\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e0942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=.9)\n",
    "prompt  = ChatPromptTemplate.from_template(\"Who won the Wimbledon for year {year}?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915421e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nComputer: Novak Djokovic won the 2021 Wimbledon.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cb62c",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain\n",
    "The simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b25952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7689ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=.1)\n",
    "template = \"\"\"Who is the president of the {country} ?\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"country\"], template=template)\n",
    "president_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7164c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"What is the birth date of {president}?\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"president\"], template=template)\n",
    "bdate_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42da2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[president_chain, bdate_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b9cf843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "The current President of India is Ram Nath Kovind.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Ram Nath Kovind was born on October 1, 1945.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1630c",
   "metadata": {},
   "source": [
    "## SequentialChain \n",
    "A more general form of sequential chains, allowing for multiple inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9618cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aee7ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Could you add few lines about {topic} and {subtopic}?\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\", \"subtopic\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"synopsis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a2fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarise the following synopsis to one line: {synopsis} \"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"oneLine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4937641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Translate the following synopsis to spanish:  {synopsis}\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "translated_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"translatedsynopsis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9e8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[synopsis_chain,translated_chain, summary_chain],\n",
    "    input_variables=[\"topic\", \"subtopic\"],\n",
    "    output_variables=[\"synopsis\", \"oneLine\", \"translatedsynopsis\"],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0496b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Spain',\n",
       " 'subtopic': 'Economy',\n",
       " 'synopsis': '\\n\\nSpain is a major European economy and a member of the European Union. It is the 14th largest economy in the world and the fifth largest in the European Union. The Spanish economy is highly diversified, with sectors such as tourism, manufacturing, energy, construction, and finance all contributing to its growth. Spain has a strong agricultural sector, with the production of olives, wine, and citrus fruits being particularly important. The country is also a major producer of renewable energy, with wind and solar energy being the main sources. In recent years, Spain has seen a surge in foreign investment, particularly in the technology sector, which has helped to boost the economy.',\n",
       " 'oneLine': '\\n\\nSpain is a major European economy and a member of the European Union, with a highly diversified economy and strong agricultural sector, as well as a surge in foreign investment in the technology sector.',\n",
       " 'translatedsynopsis': '\\n\\nEspaña es una economía europea importante y miembro de la Unión Europea. Es la decimocuarta economía más grande del mundo y la quinta más grande de la Unión Europea. La economía española es muy diversificada, con sectores como el turismo, la fabricación, la energía, la construcción y las finanzas contribuyendo a su crecimiento. España tiene un fuerte sector agrícola, con la producción de aceitunas, vino y cítricos siendo particularmente importantes. El país también es un gran productor de energía renovable, con la energía eólica y solar siendo las principales fuentes. En los últimos años, España ha visto un aumento de la inversión extranjera, particularmente en el sector de la tecnología, lo que ha ayudado a impulsar la economía.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain({\"topic\":\"Spain\", \"subtopic\": \"Economy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47f444",
   "metadata": {},
   "source": [
    "## LoadSummarize chain\n",
    "\n",
    "This is the fundamental chain to perform the task of summarization over documents.\n",
    "\n",
    "### Map reduce\n",
    "The map reduce documents chain first applies an LLM chain to each document individually, treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.\n",
    "\n",
    "This algorithm works by first splitting the entire input into small chunks using a text splitter. Then we create a summary of each of these chunks. Once we get a summary of each, the algorithm creates a summary over these summaries and provides an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1cb4d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ae164d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2185b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca51fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=True)\n",
    "chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6b84fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' President Biden has outlined a Unity Agenda for the Nation, which includes tackling the opioid epidemic, mental health issues, and supporting veterans. In response to Russian aggression in Ukraine, the US and its allies are taking action to seize ill-gotten gains and isolate them economically. The US is also providing military, economic, and humanitarian assistance to Ukraine. The American Rescue Plan and the Bipartisan Infrastructure Law have been passed to help Americans struggling with the pandemic and to create jobs. The President is also pushing for the passage of the Bipartisan Innovation Act, which would make record investments in emerging technologies and American manufacturing. He is encouraging the use of American products to support American jobs and is proposing a global minimum tax rate to prevent companies from avoiding taxes. Finally, he is calling for Americans to come together and move forward on COVID-19 and other issues.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run( docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa34045",
   "metadata": {},
   "source": [
    "# Router\n",
    "Router chain dynamically selects the next chain to use for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37594ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2554c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9ce7070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04640ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba5e2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "521bfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "23a91440",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "023afad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': \"What is Newton's third law?\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Newton's third law states that for every action, there is an equal and opposite reaction. In other words, the forces between two objects always come in pairs, and these forces are equal in magnitude and opposite in direction.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"What is Newtons third law?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab7f9d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'What is the largest three digit prime number?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "The largest three digit prime number is 997.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"What is the largest 3 digit prime number?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc865be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Computer: In 2019, Novak Djokovic won the Wimbledon Men's Singles title. The First World War took place primarily in Europe, with the main battles taking place in France, Belgium, and Germany.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Where did the First World War take place?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f15475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
